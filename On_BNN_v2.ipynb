{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Miguel Solinas \",\"Elvin Ugonna\"\n",
    "__credits__ = \"Miguel Solinas \",\"Elvin Ugonna\"\n",
    "__version__ = \"0.1.0\"\n",
    "__maintainer__ = \"Miguel Solinas Jr.\",\"Elvin Ugonna\"\n",
    "__email__ = \"migue.solinas@gmail.com\",\"elvindavin@gmail.com\"\n",
    "__status__ = \"Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data1.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-87ed44db8881>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#open dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data1.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data1.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#open dataset\n",
    "file = pd.read_csv(\"data1.csv\")\n",
    "dataset = pd.DataFrame(file)\n",
    "\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3f799fc678f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasss\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasss\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.classs[dataset.classs==1]=0\n",
    "dataset.classs[dataset.classs==2]=1\n",
    "dataset.to_csv(\"dataset.csv\")\n",
    "dataset.describe()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa17667bf5f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Compute the correlation matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Generate a mask for the upper triangle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = dataset.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "\n",
    "\n",
    "print(\" 1) Some of the featues do not deliver major information\\n\") \n",
    "print(\" 2) The feature type is usefull, it contains just one value (4)\\n\")\n",
    "print(\" 3) Pos/2 is not correlated to any feature and it does not change along the samples it is always close to one value\\n \")\n",
    "print(\" 4) I can not ensure if there is a correlation between the feature class and the rest ot the features \\n\")\n",
    "print(\" 5) Dataset not well balanced clas1 = \", np.count_nonzero(dataset.classs==0), \"class2 =\",np.count_nonzero(dataset.classs==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Descrpition:\n",
    "#  Set of normalization functions\n",
    "#  Normalize each columns of the dataset, this function accept a normalize function like argument (lambda : function()).\n",
    "#  Args:\n",
    "#   dataframe: pandas DataFrame of features\n",
    "#   Normalize function: function to be implemented to normalize the dataset\n",
    "#  Returns:\n",
    "#    A version of the input `DataFrame` that has all its features normalized following the specified function.\n",
    "\n",
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
    "\n",
    "def z_score_normalize(series):\n",
    "  mean = series.mean()\n",
    "  std_dv = series.std()\n",
    "  return series.apply(lambda x:(x - mean) / std_dv)\n",
    "\n",
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "  boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "  quantiles = feature_values.quantile(boundaries)\n",
    "  return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def min_max(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    max_ = np.max(dataset)\n",
    "    min_ = np.min(dataset)\n",
    "    return (dataset - mu)/(max_-min_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to split the dataset into three (validation, training and testing)\n",
    "# They must to be the same everytime, we can respect that by choosing a random seed (in this case 1)\n",
    "\n",
    "def split_dataset (dataframe,validation=False,seed=1):\n",
    "    np.random.seed(seed) \n",
    "    mask1 = np.random.randn(len(dataframe))<0.7\n",
    "    training = dataframe[mask1]\n",
    "    no_training = dataframe[~mask1]\n",
    "    if validation :\n",
    "        mask2 = np.random.randn(len(no_training))>0.5\n",
    "        validation = no_training[mask2]\n",
    "        testing = no_training[~mask2] # ~ gives the complement of a binary chipher from 0100 to 1011 (https://data-flair.training/blogs/python-operator/)\n",
    "        return training, validation, testing\n",
    "    else:\n",
    "        print(\"validation equals to testing\")\n",
    "        return training, no_training,no_training\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maturrin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.data import Dataset\n",
    "#The tf.data API enables you to build complex input pipelines from simple, reusable pieces\n",
    "\n",
    "\n",
    "class dataset_setup ():\n",
    "\n",
    "    def __init__(self,features,targets):\n",
    "        self.allDataset_features = None\n",
    "        self.allDataset_targets = None\n",
    "        self.training_features = None\n",
    "        self.training_targets = None\n",
    "        self.validation_features = None\n",
    "        self.validation_targets= None\n",
    "        self.testing_features = None\n",
    "        self.testing_targets= None\n",
    "        self.targets= targets\n",
    "        self.features= features\n",
    "        self.targets_aux= None\n",
    "        self.features_aux= None\n",
    "        \n",
    "    def input_data(self,path_file,sep=',',header = 0):\n",
    "        \n",
    "        dataset_original = pd.read_csv(path_file, sep=sep, header=header)\n",
    "#         pd.options.display.float_format = '{:.4f}'.format\n",
    "        \n",
    "        dataset = pd.DataFrame(dataset_original.reindex(\n",
    "            np.random.permutation(dataset_original.index)))\n",
    "        \n",
    "        #Normalization\n",
    "        dataset[features] = min_max(dataset[features]) # we need to normalize just the features not the labels\n",
    "        #Splitting dataset into three subsets    \n",
    "        ds_training, ds_validation, ds_testing = split_dataset(dataset,validation =True)\n",
    "        \n",
    "        print(\"Training samples \",ds_training.shape,\"\\nValidation samples\",ds_validation.shape,\"\\nTesting samples\",ds_testing.shape)\n",
    "        #All dataset features and targets\n",
    "        self.allDataset_features = dataset[self.features]\n",
    "        self.allDataset_targets = dataset[self.targets]\n",
    "        #Training dataset\n",
    "        self.training_features = ds_training[self.features]\n",
    "        self.training_targets = ds_training[self.targets]\n",
    "        #Validation dataset\n",
    "        self.validation_features =  ds_validation[self.features]\n",
    "        self.validation_targets = ds_validation[self.targets]\n",
    "        #Testing dataset\n",
    "        self.testing_features = ds_testing[self.features]\n",
    "        self.testing_targets = ds_testing[self.targets]\n",
    "\n",
    "# Name: \n",
    "#  pipeline_feeder\n",
    "# Description:\n",
    "#  The function will receive a couple of features/targets (dataframes) that will be splitted into a specific\n",
    "#  number of epoch. Each epoch will containt as samples as the batchsize specified.\n",
    "# Args:\n",
    "#  features\n",
    "#   type  -> DataFrame\n",
    "#   features -> It is expected a dataframe with the columns of the features and the respective data \n",
    "#   targets  -> It is expected a dataframe with the labels. labels can be coded in one-hote or discrete \n",
    "#           -> n° labels = n° targets  they must contain the same index position (Sample 1 feature -> sample 1 target) \n",
    "#  Batch_size:\n",
    "#   type  -> Integer\n",
    "#   It defines the size of the batch, if batch_size is None the dataset will not be splitted (Batch gradient vs mini-batch (S.G.D))\n",
    "#  Features_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra features to be splitted if it is required \n",
    "#  targets_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra target to be splitted if it is required # shuffle\n",
    "#  Shuffle:\n",
    "#   type  -> Boolean\n",
    "#   The batch can be shuffled to increase randomization\n",
    "#  num_epochs:\n",
    "#   type  -> Integer\n",
    "#   It defines the times that a batch must be loaded in memory\n",
    "# Returns:\n",
    "#  A  couple of target/features iterators\n",
    "\n",
    "    def pipeline_feeder(self,features, targets, batch_size,features_aux= None,targets_aux=None, shuffle = False, num_epochs=None):\n",
    "\n",
    "\n",
    "        # warning: 2GB limit\n",
    "        if features_aux is not None: \n",
    "            features_new = [features,features_aux] #we create a list containing extra features or extra targets\n",
    "            features = pd.concat(features_new,1) #we concatenate the extra information with the original one alongisde index 1 \n",
    "        elif targets_aux is not None:\n",
    "            targets_new = [features,features_aux]\n",
    "            target = pd.concat(targets_new,1)\n",
    "\n",
    "        print(features.shape, targets.shape)\n",
    "        ds = Dataset.from_tensor_slices((features, targets))  # We call dataset function from the API data that allow us to create\n",
    "                                                              # tensor objects to feed the computational graph.\n",
    "\n",
    "        # Shuffle the data, if specified\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "\n",
    "        if batch_size == 0:\n",
    "            ds = ds.batch(features.shape[0]).repeat(num_epochs)\n",
    "        else:\n",
    "            ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "        # Return the next batch of data\n",
    "        features, labels = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "        # labels= tf.reshape(labels,shape=(batch_size,1))\n",
    "        return features, labels\n",
    "        # return ds.make_one_shot_iterator()\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_training_batch(self,batch_size,num_epochs,features=None):\n",
    "\n",
    "        training_batch = self.pipeline_feeder(self.training_features,\n",
    "                                           self.training_targets,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_epochs=num_epochs,\n",
    "                                                   shuffle=True )\n",
    "        return training_batch\n",
    "\n",
    "    def get_testing_batch(self,num_epochs,features=None):\n",
    "        testing_batch =  self.pipeline_feeder(self.testing_features,\n",
    "                                                   self.testing_targets,\n",
    "                                                           num_epochs=num_epochs,\n",
    "                                                           batch_size=self.testing_targets.shape[0], # All the testing set\n",
    "                                                           shuffle=False)\n",
    "        return testing_batch\n",
    "\n",
    "    \n",
    "    def get_validation_batch(self,num_epochs,features=None):\n",
    "        validation_batch =  self.pipeline_feeder(self.validation_features,\n",
    "                                                   self.validation_targets,\n",
    "                                                           num_epochs=num_epochs,\n",
    "                                                           batch_size=self.validation_targets.shape[0], # All the validation set\n",
    "                                                           shuffle=False)\n",
    "        return validation_batch\n",
    "    \n",
    "    def get_all_dataset(self,features=None):\n",
    "        dataset_batch = self.pipeline_feeder(self.allDataset_features,\n",
    "                                                             self.allDataset_targets,\n",
    "                                                             num_epochs=1,\n",
    "                                                             batch_size=self.allDataset_targets.shape[0],\n",
    "                                                             shuffle=False)\n",
    "\n",
    "        return dataset_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features= [ 'time', 'pos/0', 'pos/1','spd/0', 'spd/1']\n",
    "features_aux= [ 'type', 'sender', 'messageID', 'pos/2','spd/0', 'spd/1', 'pos/2']\n",
    "# Most of the features are discrete and do not represent continous values\n",
    "# There is one exception with pos/2 that is continous, however it does not change, it can be eliminated \n",
    "\n",
    "targets= ['classs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset_setup(features,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples  (3050, 10) \n",
      "Validation samples (323, 10) \n",
      "Testing samples (656, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset.input_data(\"data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3050, 5) (3050, 1)\n",
      "[[ 0.18187908 -0.07851001 -0.28588466 -0.03332444  0.0124303 ]]\n",
      "[[ 0.39126259 -0.5319003  -0.06187343  0.26559908 -0.02362654]]\n",
      "[[ 0.47190416 -0.06703684 -0.30836746  0.1818891  -0.05342056]]\n",
      "[[-0.50006901  0.10855415 -0.25187078 -0.03332444  0.0124303 ]]\n",
      "[[-0.24528347 -0.28335894  0.28942064 -0.03386514  0.3649975 ]]\n",
      "[[ 0.23449413  0.11048    -0.26164314 -0.03332444  0.0124303 ]]\n",
      "[[-0.01052427 -0.31280695 -0.43148568 -0.01688538  0.10195898]]\n",
      "[[ 0.19071824  0.05520614 -0.1545439   0.28631738  0.02339907]]\n",
      "[[-0.15662801  0.34518099  0.19399679 -0.03332444  0.0124303 ]]\n",
      "[[-0.48776965 -0.27834741  0.51235021 -0.1123976  -0.42218458]]\n"
     ]
    }
   ],
   "source": [
    "features,target = dataset.get_training_batch(1,10)\n",
    "import tensorflow as tf\n",
    "\n",
    "sess =  tf.InteractiveSession()\n",
    "for i in range(10 ) : print(sess.run(features))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323, 5) (323, 1)\n",
      "(656, 5) (656, 1)\n",
      "(4029, 5) (4029, 1)\n"
     ]
    }
   ],
   "source": [
    "features,target=dataset.get_validation_batch(10)\n",
    "features,target=dataset.get_testing_batch(10)\n",
    "features,target=dataset.get_all_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
