{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Miguel Solinas \",\"Elvin Ugonna\"\n",
    "__credits__ = \"Miguel Solinas \",\"Elvin Ugonna\"\n",
    "__version__ = \"0.1.0\"\n",
    "__maintainer__ = \"Miguel Solinas Jr.\",\"Elvin Ugonna\"\n",
    "__email__ = \"migue.solinas@gmail.com\",\"elvindavin@gmail.com\"\n",
    "__status__ = \"Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type           int64\n",
       "time         float64\n",
       "sender         int64\n",
       "messageID      int64\n",
       "pos/0        float64\n",
       "pos/1        float64\n",
       "pos/2        float64\n",
       "spd/0        float64\n",
       "spd/1        float64\n",
       "classs         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open dataset\n",
    "file = pd.read_csv(\"data1.csv\")\n",
    "dataset = pd.DataFrame(file)\n",
    "\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>time</th>\n",
       "      <th>sender</th>\n",
       "      <th>messageID</th>\n",
       "      <th>pos/0</th>\n",
       "      <th>pos/1</th>\n",
       "      <th>pos/2</th>\n",
       "      <th>spd/0</th>\n",
       "      <th>spd/1</th>\n",
       "      <th>classs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4029.0</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4.029000e+03</td>\n",
       "      <td>4.029000e+03</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18050.008896</td>\n",
       "      <td>236.864483</td>\n",
       "      <td>134793.972946</td>\n",
       "      <td>4719.204751</td>\n",
       "      <td>5571.488144</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>1.797976e+00</td>\n",
       "      <td>-1.060090</td>\n",
       "      <td>1.846612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.807848</td>\n",
       "      <td>151.330926</td>\n",
       "      <td>75782.566687</td>\n",
       "      <td>1089.826240</td>\n",
       "      <td>258.697882</td>\n",
       "      <td>1.934249e-13</td>\n",
       "      <td>8.136849e+00</td>\n",
       "      <td>18.057582</td>\n",
       "      <td>0.360406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18000.006320</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>2331.525684</td>\n",
       "      <td>5180.351961</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>-1.712646e+01</td>\n",
       "      <td>-43.874673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18024.733440</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>70699.000000</td>\n",
       "      <td>3613.652104</td>\n",
       "      <td>5313.817688</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>-1.791909e+00</td>\n",
       "      <td>-6.191417</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18049.699190</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>138293.000000</td>\n",
       "      <td>4426.073097</td>\n",
       "      <td>5562.989770</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>8.530000e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18074.727730</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>201806.000000</td>\n",
       "      <td>5936.455077</td>\n",
       "      <td>5778.752997</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>5.579280e+00</td>\n",
       "      <td>2.175522</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18099.997670</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>260739.000000</td>\n",
       "      <td>6324.850321</td>\n",
       "      <td>6080.031981</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>3.682722e+01</td>\n",
       "      <td>41.408049</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type          time       sender      messageID        pos/0  \\\n",
       "count  4029.0   4029.000000  4029.000000    4029.000000  4029.000000   \n",
       "mean      4.0  18050.008896   236.864483  134793.972946  4719.204751   \n",
       "std       0.0     28.807848   151.330926   75782.566687  1089.826240   \n",
       "min       4.0  18000.006320     7.000000     153.000000  2331.525684   \n",
       "25%       4.0  18024.733440   103.000000   70699.000000  3613.652104   \n",
       "50%       4.0  18049.699190   217.000000  138293.000000  4426.073097   \n",
       "75%       4.0  18074.727730   361.000000  201806.000000  5936.455077   \n",
       "max       4.0  18099.997670   601.000000  260739.000000  6324.850321   \n",
       "\n",
       "             pos/1         pos/2         spd/0        spd/1       classs  \n",
       "count  4029.000000  4.029000e+03  4.029000e+03  4029.000000  4029.000000  \n",
       "mean   5571.488144  1.895000e+00  1.797976e+00    -1.060090     1.846612  \n",
       "std     258.697882  1.934249e-13  8.136849e+00    18.057582     0.360406  \n",
       "min    5180.351961  1.895000e+00 -1.712646e+01   -43.874673     1.000000  \n",
       "25%    5313.817688  1.895000e+00 -1.791909e+00    -6.191417     2.000000  \n",
       "50%    5562.989770  1.895000e+00  8.530000e-15     0.000000     2.000000  \n",
       "75%    5778.752997  1.895000e+00  5.579280e+00     2.175522     2.000000  \n",
       "max    6080.031981  1.895000e+00  3.682722e+01    41.408049     2.000000  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Descrpition:\n",
    "#  Set of normalization functions\n",
    "#  Normalize each columns of the dataset, this function accept a normalize function like argument (lambda : function()).\n",
    "#  Args:\n",
    "#   dataframe: pandas DataFrame of features\n",
    "#   Normalize function: function to be implemented to normalize the dataset\n",
    "#  Returns:\n",
    "#    A version of the input `DataFrame` that has all its features normalized following the specified function.\n",
    "\n",
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
    "\n",
    "def z_score_normalize(series):\n",
    "  mean = series.mean()\n",
    "  std_dv = series.std()\n",
    "  return series.apply(lambda x:(x - mean) / std_dv)\n",
    "\n",
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "  boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "  quantiles = feature_values.quantile(boundaries)\n",
    "  return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def min_max(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    max_ = np.max(dataset)\n",
    "    min_ = np.min(dataset)\n",
    "    return (dataset - mu)/(max_-min_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to split the dataset into three (validation, training and testing)\n",
    "# They must to be the same everytime, we can respect that by choosing a random seed (in this case 1)\n",
    "\n",
    "def split_dataset (dataframe,validation=False,seed=1):\n",
    "    np.random.seed(seed) \n",
    "    mask1 = np.random.randn(len(dataframe))>0.7\n",
    "    training = dataframe[mask1]\n",
    "    no_training = dataframe[~mask1]\n",
    "    if validation :\n",
    "        mask2 = np.random.randn(len(no_training))>0.5\n",
    "        validation = no_training[mask2]\n",
    "        testing = no_training[~mask2] # ~ gives the complement of a binary chipher from 0100 to 1011 (https://data-flair.training/blogs/python-operator/)\n",
    "        return training, validation, testing\n",
    "    else:\n",
    "        print(\"validation equals to testing\")\n",
    "        return training, no_training,no_training\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.data import Dataset\n",
    "#The tf.data API enables you to build complex input pipelines from simple, reusable pieces\n",
    "\n",
    "\n",
    "class dataset_setup ():\n",
    "\n",
    "    def __init__(self,features,targets):\n",
    "        self.allDataset_features = None\n",
    "        self.allDataset_targets = None\n",
    "        self.training_features = None\n",
    "        self.training_targets = None\n",
    "        self.validation_features = None\n",
    "        self.validation_targets= None\n",
    "        self.testing_features = None\n",
    "        self.testing_targets= None\n",
    "        self.targets= targets\n",
    "        self.features= features\n",
    "        self.targets_aux= None\n",
    "        self.features_aux= None\n",
    "        \n",
    "    def input_data(self,path_file,sep=',',header = 0):\n",
    "        \n",
    "        dataset_original = pd.read_csv(path_file, sep=sep, header=header)\n",
    "#         pd.options.display.float_format = '{:.4f}'.format\n",
    "        \n",
    "        dataset = pd.DataFrame(dataset_original.reindex(\n",
    "            np.random.permutation(dataset_original.index)))\n",
    "        \n",
    "        #Normalization\n",
    "        dataset[features] = min_max(dataset[features]) # we need to normalize just the features not the labels\n",
    "        #Splitting dataset into three subsets    \n",
    "        ds_training, ds_validation, ds_testing = split_dataset(dataset,validation =True)\n",
    "        #All dataset features and targets\n",
    "        self.allDataset_features = dataset[self.features]\n",
    "        self.allDataset_targets = dataset[self.targets]\n",
    "        #Training dataset\n",
    "        self.training_features = ds_training[self.features]\n",
    "        self.training_targets = ds_training[self.targets]\n",
    "        #Validation dataset\n",
    "        self.validation_examples =  ds_validation[self.features]\n",
    "        self.validation_targets = ds_validation[self.targets]\n",
    "        #Testing dataset\n",
    "        self.testing_examples = ds_testing[self.features]\n",
    "        self.testing_targets = ds_testing[self.targets]\n",
    "        \n",
    "\n",
    "# Name: \n",
    "#  pipeline_feeder\n",
    "# Description:\n",
    "#  The function will receive a couple of features/targets (dataframes) that will be splitted into a specific\n",
    "#  number of epoch. Each epoch will containt as samples as the batchsize specified.\n",
    "# Args:\n",
    "#  features\n",
    "#   type  -> DataFrame\n",
    "#   features -> It is expected a dataframe with the columns of the features and the respective data \n",
    "#   targets  -> It is expected a dataframe with the labels. labels can be coded in one-hote or discrete \n",
    "#           -> n° labels = n° targets  they must contain the same index position (Sample 1 feature -> sample 1 target) \n",
    "#  Batch_size:\n",
    "#   type  -> Integer\n",
    "#   It defines the size of the batch, if batch_size is None the dataset will not be splitted (Batch gradient vs mini-batch (S.G.D))\n",
    "#  Features_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra features to be splitted if it is required \n",
    "#  targets_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra target to be splitted if it is required # shuffle\n",
    "#  Shuffle:\n",
    "#   type  -> Boolean\n",
    "#   The batch can be shuffled to increase randomization\n",
    "#  num_epochs:\n",
    "#   type  -> Integer\n",
    "#   It defines the times that a batch must be loaded in memory\n",
    "# Returns:\n",
    "#  A  couple of target/features iterators\n",
    "\n",
    "    def pipeline_feeder(self,features, targets, batch_size,features_aux= None,targets_aux=None, shuffle = False, num_epochs=None):\n",
    "\n",
    "\n",
    "        # warning: 2GB limit\n",
    "        if features_aux is not None: \n",
    "            features_new = [features,features_aux] #we create a list containing extra features or extra targets\n",
    "            features = pd.concat(features_new,1) #we concatenate the extra information with the original one alongisde index 1 \n",
    "        elif targets_aux is not None:\n",
    "            targets_new = [features,features_aux]\n",
    "            target = pd.concat(targets_new,1)\n",
    "\n",
    "        ds = Dataset.from_tensor_slices((features, targets))  # We call dataset function from the API data that allow us to create\n",
    "                                                              # tensor objects to feed the computational graph.\n",
    "\n",
    "        # Shuffle the data, if specified\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "\n",
    "        if batch_size == 0:\n",
    "            ds = ds.batch(features.shape[0]).repeat(num_epochs)\n",
    "        else:\n",
    "            ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "        # Return the next batch of data\n",
    "        features, labels = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "        # labels= tf.reshape(labels,shape=(batch_size,1))\n",
    "        return features, labels\n",
    "        # return ds.make_one_shot_iterator()\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_training_batch(self,batch_size,num_epochs):\n",
    "\n",
    "        training_batch = self.pipeline_feeder(self.training_features,\n",
    "                                           self.training_targets,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_epochs=num_epochs,\n",
    "                                                   shuffle=True )\n",
    "        return training_batch\n",
    "\n",
    "    def get_testing_batch(self,num_epochs,features):\n",
    "        testing_batch =  self.pipeline_feeder(self.testing_features,\n",
    "                                                   self.testing_targets,\n",
    "                                                           num_epochs=num_epochs,\n",
    "                                                           batch_size=self.testing_targets.shape[0], # All the testing set\n",
    "                                                           shuffle=False)\n",
    "        return testing_batch\n",
    "\n",
    "    \n",
    "    def get_validation_batch(self,num_epochs,features):\n",
    "        validation_batch =  self.pipeline_feeder(self.validation_features,\n",
    "                                                   self.validation_targets,\n",
    "                                                           num_epochs=num_epochs,\n",
    "                                                           batch_size=self.validation_targets.shape[0], # All the validation set\n",
    "                                                           shuffle=False)\n",
    "        return validation_batch\n",
    "    \n",
    "    def get_all_dataset(self,features):\n",
    "        dataset_batch = self.pipeline_feeder(self.allDataset_labels,\n",
    "                                                             self.allDataset_targets,\n",
    "                                                             num_epochs=1,\n",
    "                                                             batch_size=self.dataset_targets.shape[0],\n",
    "                                                             shuffle=False)\n",
    "\n",
    "        return dataset_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= [ 'time', 'pos/0', 'pos/1','spd/0', 'spd/1']\n",
    "features_aux= [ 'type', 'sender', 'messageID', 'pos/2','spd/0', 'spd/1', 'pos/2']\n",
    "# Most of the features are discrete and do not represent continous values\n",
    "# There is one exception with pos/2 that is continous, however it does not change, it can be eliminated \n",
    "\n",
    "targets= ['classs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_setup(features,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.input_data(\"data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03471456  0.28632665  0.1556888  -0.00424781  0.01422699]]\n",
      "[[ 0.15530188  0.28673194  0.15589625 -0.03332444  0.0124303 ]]\n",
      "[[ 0.11633641 -0.28199881  0.14475644 -0.00236667 -0.44322714]]\n",
      "[[ 0.48993012  0.01442057 -0.1664529   0.17308354  0.04062206]]\n",
      "[[-0.23233255 -0.27533678 -0.16430775  0.03149457 -0.31865449]]\n",
      "[[-0.30021863 -0.28116671  0.39988493 -0.06969155 -0.33425445]]\n",
      "[[ 0.38230551  0.39615463 -0.15299181  0.16716671 -0.10245426]]\n",
      "[[-0.22815639 -0.14708711 -0.31075793  0.19843118  0.04934781]]\n",
      "[[-0.43665223  0.34440054  0.2631254  -0.02424923 -0.10214262]]\n",
      "[[-0.14774034  0.34509324  0.20176929 -0.03332444  0.0124303 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maturrin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "features,target = dataset.get_training_batch(1,10)\n",
    "import tensorflow as tf\n",
    "\n",
    "sess =  tf.InteractiveSession()\n",
    "for i in range(10 ) : print(sess.run(features))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
