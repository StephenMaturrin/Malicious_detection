{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Miguel Solinas \",\"Elvin Ugonna\"\n",
    "__credits__ = \"Miguel Solinas Jr.\",\"Elvin Ugonna\"\n",
    "__version__ = \"0.1.0\"\n",
    "__maintainer__ = \"Miguel Solinas Jr.\",\"Elvin Ugonna\"\n",
    "__email__ = \"migue.solinas@gmail.com\",\"elvindavin@gmail.com\"\n",
    "__status__ = \"Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>time</th>\n",
       "      <th>sender</th>\n",
       "      <th>messageID</th>\n",
       "      <th>pos/0</th>\n",
       "      <th>pos/1</th>\n",
       "      <th>pos/2</th>\n",
       "      <th>spd/0</th>\n",
       "      <th>spd/1</th>\n",
       "      <th>classs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4029.0</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4.029000e+03</td>\n",
       "      <td>4.029000e+03</td>\n",
       "      <td>4029.000000</td>\n",
       "      <td>4029.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18050.008896</td>\n",
       "      <td>236.864483</td>\n",
       "      <td>134793.972946</td>\n",
       "      <td>4719.204751</td>\n",
       "      <td>5571.488144</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>1.797976e+00</td>\n",
       "      <td>-1.060090</td>\n",
       "      <td>1.846612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.807848</td>\n",
       "      <td>151.330926</td>\n",
       "      <td>75782.566687</td>\n",
       "      <td>1089.826240</td>\n",
       "      <td>258.697882</td>\n",
       "      <td>1.934249e-13</td>\n",
       "      <td>8.136849e+00</td>\n",
       "      <td>18.057582</td>\n",
       "      <td>0.360406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18000.006320</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>2331.525684</td>\n",
       "      <td>5180.351961</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>-1.712646e+01</td>\n",
       "      <td>-43.874673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18024.733440</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>70699.000000</td>\n",
       "      <td>3613.652104</td>\n",
       "      <td>5313.817688</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>-1.791909e+00</td>\n",
       "      <td>-6.191417</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18049.699190</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>138293.000000</td>\n",
       "      <td>4426.073097</td>\n",
       "      <td>5562.989770</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>8.530000e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18074.727730</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>201806.000000</td>\n",
       "      <td>5936.455077</td>\n",
       "      <td>5778.752997</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>5.579280e+00</td>\n",
       "      <td>2.175522</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.0</td>\n",
       "      <td>18099.997670</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>260739.000000</td>\n",
       "      <td>6324.850321</td>\n",
       "      <td>6080.031981</td>\n",
       "      <td>1.895000e+00</td>\n",
       "      <td>3.682722e+01</td>\n",
       "      <td>41.408049</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type          time       sender      messageID        pos/0  \\\n",
       "count  4029.0   4029.000000  4029.000000    4029.000000  4029.000000   \n",
       "mean      4.0  18050.008896   236.864483  134793.972946  4719.204751   \n",
       "std       0.0     28.807848   151.330926   75782.566687  1089.826240   \n",
       "min       4.0  18000.006320     7.000000     153.000000  2331.525684   \n",
       "25%       4.0  18024.733440   103.000000   70699.000000  3613.652104   \n",
       "50%       4.0  18049.699190   217.000000  138293.000000  4426.073097   \n",
       "75%       4.0  18074.727730   361.000000  201806.000000  5936.455077   \n",
       "max       4.0  18099.997670   601.000000  260739.000000  6324.850321   \n",
       "\n",
       "             pos/1         pos/2         spd/0        spd/1       classs  \n",
       "count  4029.000000  4.029000e+03  4.029000e+03  4029.000000  4029.000000  \n",
       "mean   5571.488144  1.895000e+00  1.797976e+00    -1.060090     1.846612  \n",
       "std     258.697882  1.934249e-13  8.136849e+00    18.057582     0.360406  \n",
       "min    5180.351961  1.895000e+00 -1.712646e+01   -43.874673     1.000000  \n",
       "25%    5313.817688  1.895000e+00 -1.791909e+00    -6.191417     2.000000  \n",
       "50%    5562.989770  1.895000e+00  8.530000e-15     0.000000     2.000000  \n",
       "75%    5778.752997  1.895000e+00  5.579280e+00     2.175522     2.000000  \n",
       "max    6080.031981  1.895000e+00  3.682722e+01    41.408049     2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open dataset\n",
    "file = pd.read_csv(\"data1.csv\")\n",
    "dataset = pd.DataFrame(file)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Descrpition:\n",
    "#  Set of normalization functions\n",
    "#  Normalize each columns of the dataset, this function accept a normalize function like argument (lambda : function()).\n",
    "#  Args:\n",
    "#   dataframe: pandas DataFrame of features\n",
    "#   Normalize function: function to be implemented to normalize the dataset\n",
    "#  Returns:\n",
    "#    A version of the input `DataFrame` that has all its features normalized following the specified function.\n",
    "\n",
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
    "\n",
    "def z_score_normalize(series):\n",
    "  mean = series.mean()\n",
    "  std_dv = series.std()\n",
    "  return series.apply(lambda x:(x - mean) / std_dv)\n",
    "\n",
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "  boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "  quantiles = feature_values.quantile(boundaries)\n",
    "  return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def min_max(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    max_ = max(dataset)\n",
    "    min_ = min(dataset)\n",
    "    return (dataset - mu)/(max_-min_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the dataset into three (validation, training and testing)\n",
    "# They must to be the same everytime, we can respect that by choosing a random seed (in this case 1)\n",
    "\n",
    "def split_dataset (dataframe,validation=False,seed=1):\n",
    "    np.random.seed(seed) \n",
    "    mask1 = np.random.randn(len(dataframe))>0.7\n",
    "    training = ds[mask1]\n",
    "    no_training = ds[~mask1]\n",
    "    if validation :\n",
    "        mask2 = np.random.randn(len(vali_test))>0.5\n",
    "        validation = no_training[mask2]\n",
    "        testing = no_training[~mask2] # ~ gives the complement of a binary chipher from 0100 to 1011 (https://data-flair.training/blogs/python-operator/)\n",
    "        return training, validation, testing\n",
    "    else:\n",
    "        print(\"validation equals to testing\")\n",
    "        return training, no_training,no_training\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset ():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.allDataset_features = None\n",
    "        self.allDataset_targets = None\n",
    "        self.training_features = None\n",
    "        self.training_targets = None\n",
    "        self.validation_features = None\n",
    "        self.validation_targets= None\n",
    "        self.testing_features = None\n",
    "        self.testing_targets= None\n",
    "        self.targets= None\n",
    "        self.features= None\n",
    "        self.targets_aux= None\n",
    "        self.features_aux= None\n",
    "        \n",
    "    def input_data(self,path_file,sep=',',header = 0):\n",
    "        \n",
    "        dataset_original = pd.read_csv(path_file, sep=sep, header=header)\n",
    "#         pd.options.display.float_format = '{:.4f}'.format\n",
    "        \n",
    "        dataset = pd.DataFrame(TMFLT_F_dataset_original.reindex(\n",
    "            np.random.permutation(TMFLT_F_dataset_original.index)))\n",
    "        \n",
    "        #Normalization\n",
    "        dataset[features] = min_max(dataset[features]) # we need to normalize just the features not the labels\n",
    "        #Splitting dataset into three subsets    \n",
    "        ds_training, ds_validation, ds_testing = split_dataset(dataset,validation =True)\n",
    "        #All dataset features and targets\n",
    "        self.allDataset_features = dataset[self.features]\n",
    "        self.allDataset_targets = dataset[self.targets]\n",
    "        #Training dataset\n",
    "        self.training_features = ds_training[self.features]\n",
    "        self.training_targets = ds_training[self.targets]\n",
    "        #Validation dataset\n",
    "        self.training_examples =  ds_validation[self.features]\n",
    "        self.training_targets = ds_validation[self.targets]\n",
    "        #Testing dataset\n",
    "        self.validation_examples = ds_testing[self.features]\n",
    "        self.validation_targets = ds_testing[self.targets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.data import Dataset\n",
    "#The tf.data API enables you to build complex input pipelines from simple, reusable pieces\n",
    "\n",
    "# Name: \n",
    "#  pipeline_feeder\n",
    "# Description:\n",
    "#  The function will receive a couple of features/targets (dataframes) that will be splitted into a specific\n",
    "#  number of epoch. Each epoch will containt as samples as the batchsize specified.\n",
    "# Args:\n",
    "#  features\n",
    "#   type  -> DataFrame\n",
    "#   features -> It is expected a dataframe with the columns of the features and the respective data \n",
    "#   targets  -> It is expected a dataframe with the labels. labels can be coded in one-hote or discrete \n",
    "#           -> n° labels = n° targets  they must contain the same index position (Sample 1 feature -> sample 1 target) \n",
    "#  Batch_size:\n",
    "#   type  -> Integer\n",
    "#   It defines the size of the batch, if batch_size is None the dataset will not be splitted (Batch gradient vs mini-batch (S.G.D))\n",
    "#  Features_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra features to be splitted if it is required \n",
    "#  targets_aux:\n",
    "#   type  -> DataFrame\n",
    "#   Extra target to be splitted if it is required # shuffle\n",
    "#  Shuffle:\n",
    "#   type  -> Boolean\n",
    "#   The batch can be shuffled to increase randomization\n",
    "#  num_epochs:\n",
    "#   type  -> Integer\n",
    "#   It defines the times that a batch must be loaded in memory\n",
    "# Returns:\n",
    "#  A  couple of target/features iterators\n",
    "\n",
    "def pipeline_feeder(features, targets, batch_size,features_aux= None,targets_aux=None, shuffle = False, num_epochs=None):\n",
    "\n",
    "    \n",
    "    # warning: 2GB limit\n",
    "    if features_aux is not None: \n",
    "        features_new = [features,features_aux] #we create a list containing extra features or extra targets\n",
    "        features = pd.concat(features_new,1) #we concatenate the extra information with the original one alongisde index 1 \n",
    "    elif targets_aux is not None:\n",
    "        targets_new = [features,features_aux]\n",
    "        target = pd.concat(targets_new,1)\n",
    "   \n",
    "    ds = Dataset.from_tensor_slices((features, targets))  # We call dataset function from the API data that allow us to create\n",
    "                                                          # tensor objects to feed the computational graph.\n",
    "    \n",
    "    # Shuffle the data, if specified\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    if batch_size == 0:\n",
    "        ds = ds.batch(features.shape[0]).repeat(num_epochs)\n",
    "    else:\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Return the next batch of data\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "    # labels= tf.reshape(labels,shape=(batch_size,1))\n",
    "    return features, labels\n",
    "    # return ds.make_one_shot_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'time', 'sender', 'messageID', 'pos/0', 'pos/1', 'pos/2',\n",
       "       'spd/0', 'spd/1', 'classs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283 199 517   7 211 385 421 415 145  31]\n",
      "[ 91 319  43 211 307 205 199 541  97 499]\n",
      "[391  97 331  55 241  49  31 109 187 103]\n",
      "[175  61  73 313 145 349 337 145 109 187]\n",
      "[289 157 397 181  97 271 265 181 337 331]\n",
      "[103  73  61  73  37 307   7  49  73 469]\n",
      "[ 73 139 259 157  73 337   7 139 415  43]\n",
      "[ 37  31 361 169 193 229  55 163 187 205]\n",
      "[493 391 277 265 265 109  55 259 265  97]\n",
      "[307 505 571 145 529 109 253  73 331 319]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maturrin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "features,labels = pipeline_feeder(dataset.sender,dataset.classs,10,shuffle=True,num_epochs=10)\n",
    "import tensorflow as tf\n",
    "\n",
    "sess =  tf.InteractiveSession()\n",
    "for i in range(10 ) : print(sess.run(features))\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
